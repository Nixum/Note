[TOC]

# 底层原理

容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个边界。

## Namespace - 隔离

进程只能看到被规定的视图，即 隔离，比如通过docker启动一个/bin/sh，再在容器里通过ps命令查看该/bin/sh进程的pid，会发现它的pid是1，但是实际上它在外部的宿主机里的pid是10，使得让在容器里运行的进程以为自己就在一个独立的空间里，实际上只是进行了逻辑的划分，本质还是依赖宿主机。

作用：在同一台宿主机上运行多个用户的容器，充分利用系统资源；不同用户之间不能访问对方的资源，保证安全。

常见的Namespace类型有：

* PID Namespace：隔离不同容器的进程
* Network Namespace：隔离不同容器间的网络
* Mount Namespace：隔离不同容器间的文件系统

**与虚拟化的区别**：虚拟化是在操作系统和硬件上进行隔离，虚拟机上的应用需要经过虚拟机在经过宿主机，有两个内核，本身就有消耗，而容器化后的应用仅仅只是宿主机上的进程而已，只用到宿主机一个内核

因为namespace隔离的并不彻底，由于内核共享，容器化应用仍然可以把宿主机的所有资源都吃掉，有些资源不同通过namespace隔离，比如修改了容器上的时间，宿主机上的时间也会被改变，因此需要Cgroups

## Cgroups - 资源限制

是用来制造约束的主要手段，即对进程设置资源能够使用的上限，如CPU、内存、IO设备的流量等

比如，限定容器只能使用宿主机20%的CPU

```shell
docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash
```

> Cgroups 通过不同的子系统限制了不同的资源，每个子系统限制一种资源。每个子系统限制资源的方式都是类似的，就是把相关的一组进程分配到一个控制组里，然后通过树结构进行管理，每个控制组都设有自己的资源控制参数。

常见的Cgroups子系统

* CPU 子系统，用来限制一个控制组（一组进程，你可以理解为一个容器里所有的进程）可使用的最大 CPU。
* memory 子系统，用来限制一个控制组最大的内存使用量。
* pids 子系统，用来限制一个控制组里最多可以运行多少个进程。
* cpuset 子系统， 这个子系统来限制一个控制组里的进程可以在哪几个物理 CPU 上运行。

Cgroups 有 v1 和 v2 两个版本，v1中每个进程在各个Cgroups子系统中独立配置，可以属于不同的group，灵活但会导致对同一进程的资源协调困难，v2针对此做了改进，使各个子系统可以协调统一管理资源。

## Mount Namespace与rootfs(根文件系统)

挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，即容器镜像，也是容器的根文件系统。Mount Namespace保证每个容器都有自己独立的文件目录结构。

镜像可以理解为是容器的文件系统（一个操作系统的所有文件和目录），它是只读的，挂载在宿主机的一个目录上。同一台机器上的所有容器，都共享宿主机操作系统的内核，如果容器内应用修改了内核参数，会影响到所有依赖的应用。而虚拟机则都是独立的内核和文件系统，共享宿主机的硬件资源。

> 上面的读写层通常也称为容器层，下面的只读层称为镜像层，所有的增删查改操作都只会作用在容器层，相同的文件上层会覆盖掉下层。知道这一点，就不难理解镜像文件的修改，比如修改一个文件的时候，首先会从上到下查找有没有这个文件，找到，就复制到容器层中，修改，修改的结果就会作用到下层的文件，这种方式也被称为copy-on-write。

## 注意点

容器是“单进程模型”，单进程模型并不是指容器只能运行一个进程，而是指容器没有管理多个进程的能力，它只能管理一个进程，即如果在容器里启动了一个Web 应用和一个nginx，如果nginx挂了，你是不知道的。

另外，直到JDK 8u131以后，java应用才能很好的运用在docker中，在此之前可能因为docker隔离出的配置和环境，导致JVM初始化默认数值出错，因此如果使用以前的版本，需要显示设置默认配置，比如直接规定堆的最大值和最小值、线程数之类的

# 进程

Linux中的进程状态

![](https://github.com/Nixum/Java-Note/raw/master/Note/picture/Linux进程状态.jpg)

活着的进程有两种状态：

* 运行态(TASK_RUNNING)：进程正在运行，或 处于run queue队列里等待
* 睡眠态(TASK_INTERRUPTIBLE、TASK_UNINTERRUPTIBLE)：因为需要等待某些资源而被放在了wait queue队列，该状态包括两个子状态：
  * 可被打断状态(TASK_INTERRUPTIBLE)：此时ps查看的Stat的值为 S
  * 不可被打断状态(TASK_UNINTERRUPTIBLE)：此时ps查看的Stat的值为 D

进程退出时会有两个状态：

* EXIT_ZOMBIE状态：僵尸状态；之所以有这个状态是为了给父进程可以查看子进程PID、终止状态、资源使用信息的机会，如果子进程直接消失，父进程则没有机会掌握子进程具体的终止情况。

* EXIT_DEAD状态：真正结束退出时一瞬间的状态

## init进程

init进程也称1号进程，是第一个用户态进程，由它直接或间接创建了Namespace中的其他进程。Linux系统本身在启动后也是这么干的，会先执行内核态代码，然后根据缺省路径尝试执行1号进程的代码，从内核态切换到用户态，比如Systemd。

在容器中，无法使用SIGKILL(-9)和SIGTOP(19)这两个信号杀死1号进程，但对于其他kill信号(比如默认的kill信号是SIGTERM)，如果init进程注册了自己处理该信号的handler，则1号进程可以做出响应。但是，如果SIGKILL和SIGTOP信号是从Host Namespace里发出的，则可以被响应，因为此时容器里的1号进程在宿主机上只是一个普通进程。

**有时无法被kill掉的原因：**

Linux执行kill命令，实际上只是发送了一个信号给到Linux进程，**可被调度的进程**在收到信号后，一般会从 **默认行为(每个信号都有)、忽略、捕获后处理(需要用户进程自己针对这个信号做handler)**中进行操作，但SIGKILL和SIGTOP这两个信号是特权信号，不允许被自行捕获处理也无法忽略，只能执行系统的默认行为。

执行kill命令时，会调用一系列内核函数进行处理，其中有一个函数sig_task_ignored会判断是否要忽略这个信号。Linux内核里的每个Namespace里的init进程，会忽略只有默认handler的信号，即如果我们的进程有处理相关信号的handler，就可以响应。可以通过`cat /proc/{进程pid}/status | grep -i SigCgt`查看该进程注册了哪些handler。

## ZOMBIE进程

ps aux查看Linux进程，STAT里的状态是Z，ps后有defunct标记，表示该进程为僵尸进程，此时该进程不可被调度。僵尸进程是Linux进程退出状态的一种，进程处于此状态下，无法响应kill命令，虽然资源被释放，但是仍然占用着进程号。

**形成的原因：**

父进程在创建完子进程后，没有对子进程进行后续的管理。

**影响：**

Linux内核在初始化系统时，会根据CPU数目现在进程最大数量`通过/proc/sys/kernel/pid_max查看`，对Linux系统而言，容器是一组进程的集合，如果容器中的应用创建过多，就会出现fork bomb行为，不断建立新进程消耗系统资源，如果达到了Linux最大进程数，导致系统不可用。

**解决方案：**

因此对于容器，也要限制容器内的进程数量，通过pdis Cgroup来完成，限制进程数目大小。在一个容器建立后，创建容器的服务会在/sys/fs/cgroup/pids下建立一个子目录作为控制组，里面的pids.max文件表示容器允许的最大进程数目。当容器内进程达到最大限制后再起新进程，会报错`Resource temporarily unavailable`

当出现僵尸进程后，父进程可以调用wait函数（该函数是同步阻塞）或者调用waitpid函数（该函数仅在调用时检查僵尸进程，如果没有则返回，不会阻塞等待）回收子进程资源，避免僵尸进程 的产生。或者kill掉僵尸进程的父进程，此时僵尸进程会归附到init进程下，利用init进程的能力回收僵尸进程的资源。

init进程是所有进程的父进程，init进程具备回收僵尸进程的能力。

或者把容器内的init进程替换为tini进程，该进程具备自动回收收子进程的能力。

## 进程的退出

当我们停止一个容器时，比如`docker stop`，容器内的init进程会收到SIGTERM信号，而其他进程会收到SIGKILL信号，这意味着只有init进程才能注册handler处理信号实现graceful shotdown，而其他进程不行，直接就退出了。

如果想要容器内的其他进程能收到SIGTERM信号，只能在init进程中注册一个Handler，将收到的信号转发到子进程中，在init进程退出之前把子进程都停掉，子进程就不会收到SIGKILL信号了。

# CPU

![](https://github.com/Nixum/Java-Note/raw/master/Note/picture/Linux_CPU使用分类.jpg)

## CPU Cgroup

容器会使用CPU Cgroup来控制CPU的资源使用。CPU Cgroup只会对用户态us和ni、内核态sy做限制，不对wa、hi、si这些I/O或者中断相关做限制。

CPU Cgroup一般会通过一个虚拟文件系统挂载点的方式，挂载在`/sys/fs/cgroup/cpu`目录下，每个子目录为一个控制组，各个目录间是一个树状的层级关系。

对于普通调度类型，每个目录下有三个文件对应三个参数：

* cpu.cfs_period_us：CFS的调度周期，单位微秒

* cpu.cfs_quota_us：一个调度周期内该控制组被允许运行的时间，单位微秒，`CPU最大配额 = cpu.cfs_quota_us / cpu.cfs_period_us `
* cpu.shares：CPU Cgroup对控制组之间的CPU分配比例，只有当控制组间的CPU配额超过了CPU可以资源的最大值，则会启用该参数进行配额分配。

通过Linux中cpu.cfs_period_us是个固定值，Kubernetes的pod中，限制容器CPU使用率的requestCPU和limitCPU就是通过调整其余两个参数来实现。

在容器内使用top命令查看CPU使用率，显示的是宿主机的CPU使用率以及单个进程的CPU使用率，无法查到该容器的CPU使用率。

因为top命令对于单个进程读取`/proc/[pid]/stat`里面包含进程用户态和内核态的ticks数目，对于整个节点读取的是`/proc/stat`里各个不同CPU类型的ticks数目，因为这些文件不属于任何一个Namespace，因此无法读取单个容器CPU的使用率，只能通过CPU Cgroup的控制组内的cpuacct.stat参数文件计算得到，该参数文件包含了这个控制组里所有进程的内核态ticks和用户态ticks的值，带入公式即可计算得到。

> ticks是Linux操作系统中的一个时间单位，Linux通过自己的时钟周期性产生中断，每次中断会触发内核做一次进程调度，一次中断就是一个ticks
>
> utime：表示进程在用户态部分在Linux调度中获得CPU的ticks，这个值会一直累加
>
> stime：表示进程在内核态部分在Linux调度中获得CPU的ticks，这个值会一直累加
>
> HZ：时钟频率
>
> et：utime_1和utime_2这两个值的时间间隔
>
> 进程的 CPU 使用率 =((utime_2 – utime_1) + (stime_2 – stime_1)) * 100.0 / (HZ * et * CPU个数 )

## Load Average平均负载

Load Average是指Linux进程调度器中一段时间内，可运行队列里的进程平均数 + 休眠队列中不可打断的进程平均数。

所以，有可能在使用top命令时观察到 明明CPU空闲率很高，但是Load Average的数值也很高，CPU性能下降，因为此时休眠队列中有很多在等待的进程，这些进程的stat是D，这种状态可能是IO或者信号量锁的访问导致。

# 内存

**Linux进程的内存申请策略**：Linux允许进程在申请内存时overcommit，即允许进程申请超过实际物理内存上限的内存。因为申请只是申请内存的虚拟地址，只是一个地址范围，只有真正写入数据时，才能得到真实的物理内存，当物理内存不够时，Linux就会根据一定的策略杀死某个正在运行的进程，当容器内存使用率超过限制值时，容器就会出现OOM Killed。

OOM Killed的标准是通过oom_badness函数决定，通过以下两个值的乘积决定：

* 进程已经使用的物理内存页面数
* 每个进程的OOM校准值oom_score_adj，在`/proc/[pid]/oom)score_adj`文件中，该值用于调整进程被OOM Kill的几率

**Linux的内存类型：**有两类，一类是内核使用的内存，如页表、内核栈、slab等各种cache pool；另一类是用户态使用的内存，如堆内存、栈内存、共享库内存、文件读写的Page Cache。

**RSS（Resident Set Size）**：进程真正申请到的物理页面的内存大小，RSS内存包括了进程的代码段内存，堆内存、栈内存、共享库内存，每一部分RSS内存的大小可查看`/proc/[pid]/smaps`

**Page Cache：**为了提高磁盘文件的读写性能，Linux在有空闲的内存时，默认会把读写过的页面放在Page Cache里，一旦进程需要更多的物理内存，但剩余的内存不够，则会使用(page frame reclaim)这种内存页面回收机制，根据系统里空闲物理内存是否低于某个阈值，决定是否回收Page Cache占用的内存。

## Memory Cgroup

Memory Cgroup一般会通过一个虚拟文件系统挂载点的方式，挂载在`/sys/fs/cgroup/memory`目录下，每个子目录为一个控制组，各个目录间是一个树状的层级关系。每个目录有很多参数文件，跟OOM相关的有3个

* memory.limit_in_bytes：控制控制组里进程可使用的内存最大值，父节点的控制组内该值可以限制它的子节点所有进程的内存使用，Kubernetes的limit改的就是该值，而request仅在调度时计算节点是否满足。
* memory.oom_control：当控制组中的进程内存使用达到上限时，该参数决定是否触发OOM Killed，默认是会触发OOM Killed，只能杀死控制组内的进程，无法杀死节点上的其他进程。当值设置为1时，表示控制组内进程即使达到limit_in_bytes设置的值，也不会触发OOM Killed，但是这可能会影响控制组中正在申请物理内存页面的进程，造成该进程处于停止状态，无法继续运行。
* memory.usage_in_bytes：表示当前控制组里所有进程实际使用的内存总和，与limit_in_bytes的值越接近，越容易触发OOM Killed。

当发送OOM killed时，可以查看内核日志 `journalctl -k 或者 查看日志文件 /var/log/message`

Memory Cgroup只统计了RSS和Page Cache这两部分内存，两者的和等于memory.usage_in_bytes，当控制组里的进程要申请新的物理内存，但memory.usage_in_bytes已超过memory.limit_in_bytes，此时会启用page frame reclaim内存页面回收机制，回收Page Cache的内存。

判断容器真实的内存使用量，不能仅靠Memory Cgroup里的memory.usage_in_bytes，而需要memory.stat里的rss值，该值才真正反应了容器使用的真实物理内存的大小。

## Swap

容器可以使用Swap空间，但会导致Memory Cgroup失效，比如在Swap空间足够的情况下，设置容器limit为512MB，然后容器申请了1G内存，是可以申请成功的，此时容器的RSS为512MB，Swap只剩下512MB空闲。

为了解决这个问题，可以使用swapiness，参数文件在`/proc/sys/vm/swapiness`，通过它来设置Swap使用的权重，用于定义Page Cache内存和匿名内存释放的比例值，取值范围是0到100，默认是60。100表示匿名内存和Page Cache内存的释放比例是100：100；60表示匿名内存和PageCache内存的释放比例是60：140，此时PageCache内存的释放优先于匿名内存；0不会完全禁止Swap的使用，在内存特别紧张的时候才会启用Swap来回收匿名内存。

swapiness也存在于Memory Cgroup控制组中，参数文件是memory.swappiness，该值的优先级会大于全局的swappiness，但有一点跟全局的swappiness不太一样，当memory.swappiness=0时，是完全不使用Swap空间的。

通过memory.swappiness参数可以使得需要使用Swap容器和不需要使用Swap的容器，同时运行在同一个节点上。

# 存储



# 网络

容器网络一个Network Namespace网络栈包括：网卡、回环设备、路由表、iptables规则。

## 同一节点下容器间的通信

在 Linux 中能够起到虚拟交换机作用的网络设备，是网桥。它是一个工作在数据链路层的设备，主要功能是根据 MAC 地址来将数据包转发到网桥的不同端口（Port）上，因此Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信。

容器里会有一个eth0网卡，作为默认的路由设备，连接到宿主机上一个叫vethxxx的虚拟网卡，而vethxxx网卡又插在了docker0网桥上，这一套虚拟设备就叫做Veth Pair。每个容器对应一套VethPair设备，多个容器会将其Veth Pair注册到宿主机的docker0网桥上，即Veth Pair相当于是连接不同network namespace的网线，一端在容器，一端在宿主机。

网络请求实际上就是在这些虚拟设备上进行映射（经过路由表，IP转MAC，MAC转IP）和转发，到达目的地。

**同一节点内，容器间通信一般流程**：容器A往容器B的IP发出请求，请求先经过容器A的eth0网卡，发送一个ARP广播，找到容器B IP对应的MAC地址，宿主机上的docker0网桥，把广播转发到注册到其身上的其他容器的eth0，容器B收到该广播后把MAC地址发给docker0，docker0回传给容器A，容器A发送数据包给docker0，docker0接收到数据包后，根据数据包的目的MAC地址，将其转发到容器B的eth0，

**同一节点内，宿主机与容器通信一般流程**：宿主机往容器的IP发出请求，这个请求的数据包先根据路由规则到达docker0网桥，转发到对应的Veth Pair设备上，由Veth Pair转发给容器内的应用。

## 容器访问另一节点

一个节点内的容器访问另一个节点一般流程：先经过docker0网桥，出现在宿主机上，根据路由表知道目标节点是其他机器，则将数据转发到宿主机的eth0网卡上，再发往目标节点。

其实跟同一节点内，宿主机与容器通信类似，最终转化为节点间的通信。

所以当容器无法访问外网时，就可以检查docker0网桥是否能ping通，查看docker0和Veth Pair设备的iptables规则是否有异常。

## 不同节点下容器间的通信

默认配置下，不同节点间的容器、docker0网桥，是不知道彼此的，没有任何关联，想要跨主机容器通信，就需要在多主机间在建立一个公共网桥，所有节点的容器都往这个网桥注册，才能进行通信。通过每台宿主机上有一个特殊网桥来构成这个公用网桥，这个技术被称为overlay network（覆盖网络）。

常见的解决方案是Flannel、Calico等。

![](https://github.com/Nixum/Java-Note/raw/master/Note/picture/容器网络.png)



# 安全







## Dockerfile

[指令详解](https://www.cnblogs.com/panwenbin-logs/p/8007348.html)

* RUN

后面一般接shell命令，但是会构建一层镜像

要注意RUN每执行一次指令都会在docker上新键一层，如果层数太多，镜像就会太过膨胀影响性能，虽然docker允许的最大层数是127层。

有多条命令可以使用&&连接

* CMD

要注意CMD只允许有一条，如果有多条只有最后一条会生效

# 参考

极客时间-深入剖析k8s-张磊

极客时间-容器实战高手课