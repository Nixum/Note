[TOC]

# 负载均衡

## 常用算法

- 轮询：按请求的顺序分配给各个服务器，适用于各台服务器性能相同
- 加权轮询：给各个服务器附上权重值，按权重的高低分配请求，适用于各台服务器性能不同，性能高的服务器权重也高
- 加权随机轮询：随机 + 二分查找的方式负载均衡，时间复杂度为O(logn)
- 最少链接：将请求发送给当前最少连接数的服务器上
- 加权最少链接：在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数
- IP地址哈希：哈希均匀分布
- 二次随机选择轮询：适合后端节点权重一致的情况，通过两次随机算法，获取到两个节点，对比节点CPU等信息，选择最优节点；（比较流行）
- 会话保持：根据客户端IP或cookie进行会话保持，同一个客户端每次选取后端节点的IP保持一致，适用于节点保持登录验证会话的场景，比较少用。

## 探活

负载均衡器上面一般会挂一个服务的多个复制集，进行流量的负载均衡，因此需要检查这多个复制集的健康情况，保证流量能正确路由到可用节点上。

### 主动健康检查

设定一定时间间隔内对各个复制集执行ping操作，一般在获取节点数量过少的场景下才触发，避免长时间频繁的ping操作增加节点负担。

比如通过当前节点数与15分钟前的节点数的比较，当小于80%时触发主动健康检查。

### 被动健康检查

通过检查节点真实流量的响应结果，判断节点是否正常

# 服务注册与发现

## 基本功能

1. 提供服务地址与域名的映射注册、查找和更新

2. 提供多种负载均衡方案

3. 健康检查，比如心跳检测

   * **服务主动探活**：服务注册到注册中心后，定时发送续租请求到注册中心，表明自己存活

     优点：该方案可最大程度**避免IP重用**导致节点在旧服务依然存活的问题(比如k8s环境)

     缺点：造成注册中心写操作变多，特别是在强一致性的注册中心上，频繁的节点变更会导致产生大量的通知事件，在同步到多个注册中心复制集时性能不佳；另外，仍然可能发生服务虽无法对外提供服务了，但仍然可以发送续租请求；面对不同的客户端需要提供不同语言的SDK

   * **注册中心主动探活**：服务提供健康检查接口，比如/ping，注册中心定时访问验证节点存活；k8s的Pod探针机制

     优点：一定程度上解决服务主动探活不能说明服务健康的问题

     缺点：IP重用问题，比如A、B两个服务都有相同的端口和/ping接口做健康检查，但是当发生服务替换时，无法区分是哪个服务，除非加上名称检查

   * 服务外挂一个负载均衡器实现服务探活、与注册中心的通信

     优点：注册中心和服务均不需要主动探活，均交由负载均衡器实现

     缺点：需要外挂的负载均衡器，增加成本

4. 注册中心需要保证CP或者AP

5. 将上述功能包装成SDK，简化使用

## 可能遇到的问题

1. 注册中心故障时，服务注册功能失效，服务也无法执行扩容操作，因此会在服务内缓存服务注册表，保证服务可通信；
2. 优先使用缓存的服务注册表，当接收到的服务注册表的节点过少时，放弃使用；
3. 通过在负载均衡器中加入被动健康检查和主动健康检查来剔除服务注册表中失效的节点，保证服务注册表的可用性
4. 参考Service Mesh的Envoy设计，不完全信任注册中心推送过来的服务注册表；

| 发现状态 | 健康检查成功 | 健康检查失败         |
| -------- | ------------ | -------------------- |
| 发现     | 路由         | 不路由               |
| 未发现   | 路由         | 不路由、剔除失败节点 |

5. 面对节点和服务频繁变更，导致广播风暴的场景，可以通过合并设定时间内产生的消息后再进行推送，目的是为了减少广播次数，但是这样会影响消息的时效性，要注意设定的时间不宜过长。

## 常见的注册中心区别

| 特征           | Nocas                         | Eureka | Zookeeper  | Consul               | ETCD   |
| -------------- | ----------------------------- | ------ | ---------- | -------------------- | ------ |
| 一致性协议     | AP或CP                        | AP     | CP         | CP                   | CP     |
| 健康检查       | TCP、HTTP、MySQL、Client Beat | TTL    | Keep Alive | TCP、HTTP、gRPC、Cmd | TTL    |
| 网络异常保护   | 支持                          | 支持   | 不支持     | 支持                 | 不支持 |
| 雪崩保护       | 支持                          | 支持   | 不支持     | 不支持               | 不支持 |
| 自动注销实例   | 支持                          | 支持   | 支持       | 不支持               | 支持   |
| 访问协议       | HTTP、DNS                     | HTTP   | TCP        | HTTP、DNS            | HTTP   |
| 跨注册中心同步 | 支持                          | 不支持 | 不支持     | 支持                 | 不支持 |
| K8s集成        | 支持                          | 不支持 | 不支持     | 支持                 | 支持   |
| 语言实现       | Java                          | Java   | Java       | GO                   | GO     |

一般来说，注册中心对一致性的要求不是很高，因为节点的注册和反注册后通知到客户端也需要时间；对可用性的优先级较高。

这里再说下K8s提供的默认服务发现（1.12后默认使用coreDNS），通过为Pod挂一个Service 或者 Pod 定义了hostname + subdomain，k8s也会为其生成Pod的 DNS A记录，然后 k8s 会把 Service 或 Pod 产生的DNS记录写入 CoreDNS 的 cache 或者 ETCD 中，同时在Pod的/etc/resolv.conf文件中添加CoreDNS服务的访问配置，Pod即可通过名称进行访问（同一命名空间下可以直接使用Pod名称进行访问，不同命名空间需要 Pod名称.命名空间名称 访问，或者直接使用Service的DNS名称访问）。

CoreDNS会监听集群内所有Service API，当服务不可用时移除记录，在新服务创建时插入新记录，这些记录会存储在CoreDNS的cache或者ETCD中。

参考：https://github.com/kubernetes/dns/blob/master/docs/specification.md

比如有如下 /etc/resolv.conf文件

```sh
# /etc/resolv.conf
nameserver 10.100.0.10  # coreDNS的IP
search cafe.svc.cluster.local svc.cluster.local cluster.local us-west-2.compute.internal
options ndots:5
```

其含义是：DNS 服务器为 10.100.0.10，当查询关键词中 `.` 的数量少于 5 个，则根据 search 中配置的域名进行查询，当查询都没有返回正确响应时再尝试直接查询关键词本身。

比如执行 host -v cn.bing.com 时会看到下面即此查询

```sh
Trying "cn.bing.com.cafe.svc.cluster.local"
Trying "cn.bing.com.svc.cluster.local"
Trying "cn.bing.com.cluster.local"
Trying "cn.bing.com.us-west-2.compute.internal"
Trying "cn.bing.com"
...
```

# 分布式事务

## 一、2PC

需要有协调者和参与者，协调者负责调度，参与者负责执行，分两步完成，1：prepare阶段 2：commit阶段。

2PC是强一致性的，保证原子性和隔离性。在执行阶段，节点是处于阻塞状态，直到commit阶段完成，本地事务才会释放资源，因此性能不佳，一般用在强一致性、并发量不大的场景。

### 正常情况下

**prepare阶段**：协调者向参与者A、B发送请求执行操作，参与者A、B开启事务，执行操作，但不commit，操作完成后，告诉协调者已经完成。

**commit阶段：**协调者收到参与者的完成响应，向参与者A、B发送commit请求，参与者A、B收到commit请求后，提交事务，完成操作；如果收到执行失败的响应，则发送回滚请求给参与者A、B，执行回滚。

### 异常情况下

在协调者等待参与者的完成响应时，协调者或参与者可能宕机，最终会导致数据不一致或阻塞，例如

**场景：**当处于prepare阶段和commit阶段之间时，协调者挂掉或重启，会导致协调者收不到参与者的响应，此时协调者就不清楚接下来的commit要发送什么请求过去，或者就不发请求过去了，导致参与者一直阻塞

**解决办法：**协调者维护一份事务日志，以方便宕机重启后恢复原来的状态，但无法为参与者设置超时自动操作，因为它并不知道commit阶段自己要进行commit还是回滚

**场景：**当处于prepare阶段和commit阶段之间时，参与者挂掉后，接收不到协调者的请求，不知道接下来要执行commit还是回滚，协调者也无法在参与者挂掉后进行回滚操作

**解决办法：**3pc

## 二、3PC

3pc实际上就是将commit阶段拆成两步，preCommit相当于一次保险阶段，作用类似于2pc的二阶段，但是它不是正真的提交

### 正常情况下

**canCommit阶段：**协调者向参与者发送请求，参与者开启事务执行操作，成功完成后响应Yes，否则响应No

**preCommit阶段：**协调者收到所有参与者的Yes响应，发送操作请求给所有参与者，告诉所有参与者进行预提交状态

**commit阶段：**协调者收到所有参与者的应答响应，向所有参与者发送commit请求，参与者收到后提交事务

### 异常情况下

如果在preCommit阶段到commit阶段之间，协调者挂了，参与者会在超时后进行事务提交

## 三、本地消息表

适合解决分布式最终一致性问题。

### 正常情况下

参与者A正常进行数据库事务并提交，将涉及到对参与者B的操作记录到本地消息表中，相当于一条日志，然后再用一个异步服务，读取该条日志控制参与者B进行相关操作，如果失败了直接重试即可，保证参与者A与B数据最终一致性。注意：参与者A的数据库操作与日志记录是一个原子性操作。

**异步消费操作可以利用RocketMQ事务**

事务性消息：本地事务和发送消息是原子性操作

https://www.jianshu.com/p/53324ea2df92

http://blog.itpub.net/31556438/viewspace-2649246/

## 四、TCC

补偿事务，每一个操作都要有对应的确认和补偿，类似于2pc，但2pc在于DB层面，TCC在于业务层面，每个业务逻辑都需要实现try-confirm-cancel的操作

**Try阶段**：对于操作的数据行，增加字段表示其状态，表示正在操作

**confirm阶段**：将try阶段中表示数据状态的字段修改为确认状态，表示已经完成操作，操作需要幂等

**cancel阶段**：将try阶段进行的操作进行回滚

通过不断重试，并发的时候还是需要分布式锁

## 五、SAGA

利用状态机实现



## 阿里云分布式事务GTS






## 参考：

[2pc、3pc](https://zhuanlan.zhihu.com/p/21994882)

[TCC](https://juejin.im/post/5bf201f7f265da610f63528a)

[TCC](https://yemablog.com/posts/tcc-1)

[华为的servicecomb](https://blog.csdn.net/weixin_42075590/article/details/89236625)

[蚂蚁金服的seata分布式事务架构](https://www.sofastack.tech/blog/sofa-meetup-3-seata-retrospect/)

[分布式事务最经典的七种解决方案](https://segmentfault.com/a/1190000040321750)

[一个go的分布式事务框架DTM](https://github.com/yedf/dtm)

# 分布式理论

## CAP理论

CAP特性

* C：Consistency，一致性，数据状态转化一致，写操作完成后的读操作，可以获取到最新的值
* A：Availability，可用性，指的是服务一直可用，可以正常响应
* P：Partition tolerance，分区容错，指的是当有节点故障不连通时，就会分区，但仍然能对外提供服务

矛盾在于这三个特性不能同时满足，比如

> 当分布式集群内有两个主从服务发生网络故障，但此时服务仍然可以访问，此时具有分区容错性。
>
> 当对主服务对数据进行修改时，由于网络问题，无法同步到从服务，当访问到从服务时，无法获取到最新的值，此时满足可用性，但是无法满足一致性。
>
> 当主从服务间网络恢复，写操作的数据虽然能在服务间同步了，但还未同步完成，此时访问从服务无法获取最新值，此时满足了一致性，但是无法满足可用性。
>
> 简单概括，只要满足分区容错，就会设置复制集，复制集同时也保证了可用，但是复制集又会有数据同步，此时又有一致性问题

所以，一般只会满足其中两个

> 1、满足CA舍弃P，也就是满足一致性和可用性，舍弃容错性。但是这也就意味着你的系统不是分布式的了，因为涉及分布式的想法就是把功能分开，部署到不同的机器上。
>
> 2、满足CP舍弃A，也就是满足一致性和容错性，舍弃可用性。如果你的系统允许有段时间的访问失效等问题，这个是可以满足的。就好比多个人并发买票，后台网络出现故障，你买的时候系统就崩溃了。
>
> 3、满足AP舍弃C，也就是满足可用性和容错性，舍弃一致性。这也就是意味着你的系统在并发访问的时候可能会出现数据不一致的情况。

所以为了分布式服务能正常使用，一般时会满足分区容错性和可用性，在一致性上不追求强一致性，而是一个逐渐一致的过程。

## BASE理论

BASE理论是对CAP三者均衡的结果，基于CAP理论演化而来，通过牺牲强一致性来获得高可用。

* Basically Available（基本可用）: 允许暂时不可用，比如访问时可以等待返回，服务降级，保证核心可用等。
* Soft state（软状态）: 允许系统存在中间状态，而该中间状态不会影响系统整体可用性，比如允许复制集副本间的数据存在延时，数据库的数据同步过程。
* Eventually consistent（最终一致性）: 系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

与数据库ACID类似，只是强度减弱了

参考：[CAP 定理的含义](http://www.ruanyifeng.com/blog/2018/07/cap.html)

# ZooKeeper

ZooKeeper保证的是CP，不保证每次服务请求的可用性，在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。另外在进行leader选举时集群都是不可用，所以说，ZooKeeper不能保证服务可用性。

## 使用场景

* 集群管理，监控节点存活状态
* 主节点选举，当服务以master-salve模式进行部署，当主节点挂掉后选出新的主节点
* 服务发现
* 分布式锁，提供独占锁、共享锁
* 分布式自增id
* 搭配Kafka、dubbo等使用

## 特点

* 顺序一致性：同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。
* 原子性：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。
* 单一系统映像：无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。
* 可靠性：一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。

## 数据模型

类似文件系统，根节点为 / ，每创建一个节点会从根节点开始挂，树形结构，每个数据节点称为znode，可以存储数据，每个znode还有自己所属的节点类型和节点状态

> - 持久节点：一旦创建就一直存在，直到将其删除。
> - 持久顺序节点：一个父节点可以为其子节点 **维护一个创建的先后顺序** ，这个顺序体现在 **节点名称** 上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数。
> - 临时节点：临时节点的生命周期是与 **客户端会话** 绑定的，**会话消失则节点消失** 。临时节点 **只能做叶子节点** ，不能创建子节点。
> - 临时顺序节点：父节点可以创建一个维持了顺序的临时节点(和前面的持久顺序性节点一样)。

## ZAB协议

通过ZAB协议保证注册到ZooKeeper上的主从节点状态同步，该协议有两种模式

* 崩溃恢复

  当整个 Zookeeper 集群刚刚启动或者Leader服务器宕机、重启或者网络故障导致**不存在过半的服务器与 Leader 服务器保持正常通信时，所有服务器进入崩溃恢复模式**，首先选举产生新的 Leader 服务器，然后集群中 Follower 服务器开始与新的 Leader 服务器进行数据同步。

* 消息广播

  当集群中超过半数机器与该 Leader 服务器完成数据同步之后，退出恢复模式进入消息广播模式，Leader 服务器开始接收客户端的事务请求生成事物提案（超过半数同意）来进行事务请求处理。

### 选举算法和流程

ZooKeeper集群机器要求至少三台机器，机器的角色分为Leader、Follower、Observer

默认使用FastLeaderElection算法，比如现在有5台服务器，每台服务器均没有数据，它们的编号分别是1, 2, 3, 4, 5按编号依次启动，它们的选择举过程如下：

1. 服务器1启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器1的状态一直属于Looking。
2. 服务器2启动，给自己投票，同时与之前启动的服务器1交换结果，由于服务器2的编号大所以服务器2胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是Looking。
3. 服务器3启动，给自己投票，同时与之前启动的服务器1,2交换信息，由于服务器3的编号最大所以服务器3胜出，此时投票数正好大于半数，所以服务器3成为leader，服务器1,2成为Follower。
4. 服务器4启动，给自己投票，同时与之前启动的服务器1,2,3交换信息，尽管服务器4的编号大，但之前服务器3已经胜出，所以服务器4只能成为Follower。
5. 服务器5启动，后面的逻辑同服务器4成为Follower。

当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的Leader服务器。

1. Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准Leader。
2. Discovery（发现阶段）：在这个阶段，Followers 跟准 Leader 进行通信，同步 followers 最近接收的事务提议。
3. Synchronization（同步阶段）:同步阶段主要是利用 Leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 Leader。
4. Broadcast（广播阶段） 到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 Leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。

## 通知机制

客户端会对某个znode建立一个watcher事件，当该znode发生变化时，这些客户端会收到ZooKeeper的通知，然后客户端根据znode的变化来做出相应的改变，类似观察者模式

# 分布式锁

## 利用数据库唯一约束

在数据库新建一个锁表，然后通过操作该表中的数据来实现，表的字段为客户端ID、加锁次数、资源的key，加锁次数 + 客户端ID可以判断是否可重入。

当我们要锁住某个方法或资源的时候，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。在insert前先query判断是否存在该记录。

优点：简单，方便理解，且不需要维护额外的第三方中间件(比如Redis,Zk)。

缺点：虽然容易理解但是实现起来较为繁琐，需要自己考虑锁超时，加事务等等。性能局限于数据库，一般对比缓存来说性能较低。对于高并发的场景并不是很适合。

## ETCD实现

ETCD采用raft算法，实现了数据的强一致性，不会出现Redis那种主机宕机，从机和主机数据不一致的情况。

- Lease 功能。可以保证分布式锁的安全性，为锁对应的 key 配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放，同时也支持续约。
- watch功能。在实现分布式锁时，如果抢锁失败，可通过 Prefix 机制返回的 KeyValue 列表获得 Revision 比自己小且相差最小的 key（称为 pre-key），对 pre-key 进行监听，因为只有它释放锁，自己才能获得锁，如果 Watch 到 pre-key 的 DELETE 事件，则说明pre-ke已经释放，自己已经持有锁。
- prefix功能。例如，一个名为 /mylock 的锁，两个争抢它的客户端进行写操作，实际写入的 key 分别为：key1=”/mylock/UUID1″，key2=”/mylock/UUID2″，其中，UUID 表示全局唯一的 ID，确保两个 key 的唯一性。很显然，写操作都会成功，但返回的 Revision 不一样，通过前缀 /mylock 查询，返回包含两个 key-value 对的的 KeyValue 列表，同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁，如果抢锁失败，则等待锁释放（对应的 key 被删除或者租约过期），然后再判断自己是否可以获得锁。并配合上一条的watch功能使用。
- reversion功能。如上一条所述，每个 key 带有一个 Revision 号，每进行一次事务加一，因此它是全局唯一的，如初始值为 0，进行一次 put(key, value)，key 的 Revision 变为 1；同样的操作，再进行一次，Revision 变为 2；换成 key1 进行 put(key1, value) 操作，Revision 将变为 3。多线程获取锁时，通过比较reversion的大小即可知道获取的顺序，避免"惊群效应"。

优点：

- 可以通过lease功能和结点健康监测，确保客户端崩溃时，锁一定会被释放。
- ETCD客户端JETCD有相应锁实现，是否满足需求需要进一步查看源码。

缺点：

- 强一致性带来的必定是写效率上的降低

## ZooKeeper实现

> 因为创建节点的唯一性，我们可以让多个客户端同时创建一个临时节点，创建成功的就说明获取到了锁 。然后没有获取到锁的客户端也像上面选主的非主节点创建一个 watcher 进行节点状态的监听，如果这个互斥锁被释放了（可能获取锁的客户端宕机了，或者那个客户端主动释放了锁）可以调用回调函数重新获得锁。
>
> 共享锁：规定所有创建节点必须有序，当你是读请求（要获取共享锁）的话，如果 **没有比自己更小的节点，或比自己小的节点都是读请求** ，则可以获取到读锁，然后就可以开始读了。**若比自己小的节点中有写请求** ，则当前客户端无法获取到读锁，只能等待前面的写请求完成
>
> 排他锁：如果你是写请求（获取独占锁），若 **没有比自己更小的节点** ，则表示当前客户端可以直接获取到写锁，对数据进行修改。若发现 **有比自己更小的节点，无论是读操作还是写操作，当前客户端都无法获取到写锁** ，等待所有前面的操作完成

## Redis实现

### 单机Redis实现

使用setnx命令，setnx命令表示如果key不存在，则可以set成功，返回1，否则返回0，根据返回值来判断是否加锁成功，注意setnx不支持设置key、value的同时还要设置过期时间，过期时间主要是保证资源占用时间过长后可以释放锁，避免死锁，所以如果要用来加锁，必须使用Lua脚本来保证原子性

```lua
if redis.call('setnx', KEYS[1], KEYS[2]) == 1 then
  redis.call('expire', KEYS[1], KEYS[3])
end

return ok
```

不过从2.6.12起，set涵盖了setex、setnx功能，并且set本身可以设置过期时间，因此可以使用以下命令进行加锁

命令：`SET [key: 资源代表的key] [value: 客户端事务Id] NX PX [时间，EX的单位是秒，PX的单位是毫秒] `，要注意旧版本的setnx命令不支持设置过期时间

解锁，先判断锁是不是自己加的，如果是才可以解锁，即删除该key，需要保证这两个步骤的原子性，否则可能会出现因为查询时间过长导致删掉了别人的锁

```lua
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

为了保证Redis的高可用，就会部署Redis集群，但在集群环境下还使用上述方案可能会出现锁偶尔失效。比如Redis设置主从节点，一般加锁解锁会在主节点上进行，但是当加锁后，主节点宕机，从节点还未进行同步，其他服务就有机会获取到锁了，此时就出现了多个服务对同一资源的操作问题。

### 集群Redis实现

RedLock与Redission实现，有空再进行补充，先把RedLock的基本思想简单描述一下

假设有5个Redis节点，

1. 加锁时，顺序再5个节点上申请锁，使用同样的key、value、超时时间，申请锁的超时时间，因为需要同时请求多个节点，避免加锁时间花费过长
2. 当在3个节点上成功申请到锁，且申请锁消耗的时间 小于 锁的有效时间，就算申请成功。申请锁消耗的时间采用获得锁的当下时间减去加锁请求时的时间戳得到
3. 锁申请到后，锁的有效时间 = 锁的过期时间 - 申请锁获得的时间
4. 如果申请锁失败了，申请成功锁的节点会执行解锁操作，进行重置

参考：

[RedLock算法的争议](http://zhangtielei.com/posts/blog-redlock-reasoning.html)

# 分布式自增id

## 数据库自增id

利用MySQL的自增id来实现，服务在需要用到自增id时，会向MySQL发送请求。通过事务的方式，先增长id再进行查询

优点：简单方便

缺点：性能不是很好，还有一个是可用性问题，如果数据库不可用了，会导致其他服务不可用，如果使用主从方式部署，虽然可靠性提升了，但如果主库挂掉后，从库数据没有及时同步，会出现ID重复

解决：使用双主模式，两个主节点的自增序列错开，比如设置节点A的起始值为1，步长为2，节点B的起始值为2，不出为2。但是这种方案又有一个缺点，那就是扩展性不好，假如两个节点不够用了，第三个节点加入时的起始值和步长不好设置

解决：客户端通过号段的方式来获取自增id，每次从数据库获取时不再是获取一个，而是获取一段范围内的一批id，缓存在本地，减少IO和竞争

## 雪花算法

分布式ID固定是一个long类型的数字，占64位，通过一定的规则编排这64位来实现

```
-------------------------------------------------------------------
|  bit   | 66 |           65 ~ 24        |  23 ~ 13  |   12 ~ 1   |
| length | 1  |             41           |    10     |     12     |
|  part  |none|         timestamp        |  machine  |  sequence  |
-------------------------------------------------------------------
41位的时间戳可以使用69年，41位的时间戳是精确到毫秒级别的
12位的序列号可以让同一个节点一毫秒内生产4096个ID
```

一般可以自己调整时间戳、机器id、序列号的位数来控制ID的并发量、使用年限等，满足不同的场景，机器ID可以自己配置在节点上、配置中心、数据库

优点：简单方便，整型易操作，有需要还可以将整型进行进制转化，转成字符串类型的id

缺点：因为使用到了时间，如果节点上的时钟回拨，会导致ID重复；如果服务部署在docker内，获取时间和机器id要注意；由于机器不同，生成的id是自增的，但是不一定连续；而且还有个问题，前端js是没有long类型的，整型最多支持53位，所以如果返回给前端还要做一次转化。

## 利用ETCD

etcd 能满足不会丢失的，多副本，强一致的全部需求。两种方案：

1. 利用ETCD实现分布式锁，对存储在ETCD中的自增ID加锁实现

2. 利用ETCD的boltdb，boltdb是一个单机支持事务的KV存储，key是reversion，value是key-value，bolted会把每个版本都保存下来，实现多版本机制

   ```
   用etcdctl通过批量接口写入两条记录：
   
   etcdctl txn <<<'
     put key1 "v1"
     put key2 "v2"
   
   再通过批量接口更新这两条记录：
   etcdctl txn <<<'
     put key1 "v12"
     put key2 "v22"
   
   boltdb中其实有了4条数据：
   rev={3 0}, key=key1, value="v1"
   rev={3 1}, key=key2, value="v2"
   rev={4 0}, key=key1, value="v12"
   rev={4 1}, key=key2, value="v22"
   
   此时可以发现reversion由两部分组成，第一部分为main rev，每次事务进行会加一，第二部分sub rev，同一个事务中每次操作加一。另外ETCD提供了命令和选项来控制存储的空间问题
   ```

## 利用ZooKeeper

利用ZooKeeper的顺序一致性和原子性，当客户端每次需要自增id时，创建一个持节顺序节点，ZooKeeper为了保证有序，会给这些节点编号，同时ZooKeeper会保证并发时不会产生冲突，创建成功后会返回类似/root/generateid0000000001的结果，再进行截取即可，另外，为了保证不浪费空间，可以用完该znode后进行删除。

缺点：ZooKeeper是CP，不保证完全高可用，另外，由于数据需要再ZooKeeper间进行过半数同步完才算写入成功，性能也一般

## 利用Redis

使用Redis的incr命令来实现原子性的自增和返回

优点：简单方便

缺点：需要对id进行持久化，虽然Redis本身提供了RDB和AOF，但如果宕机了，仍然有可能出现重复ID，如果为Redis搭建集群，由于Redis主从复制时异步复制的，无法保证master宕机之前将最新的id同步给其他子节点，导致宕机恢复之后可能会产生重复的id，需要针对这种情况做特殊处理，比如当id重复时报特殊错误码，跳过这个错误的id区间

# 分布一致性算法

## Paxos算法



## Raft算法

感觉像与ZAB协议类似，具体有时间再整理

当Leader宕机之后，只有拥有最新日志的Follower才有资格称为Leader。

关于日志检查，当Follower上的日志与leader不一致时，Leader先找到Follower同它日志一致的index，将其后边的日志一条条覆盖在Follower上。

参考：[Raft算法详解](https://zhuanlan.zhihu.com/p/32052223)

[Raft动画演示](https://raft.github.io/raftscope/index.html)

## Gossip协议

简单来讲，就是一个节点会周期、随机的将事件广播给其他节点，其他节点收到后也会周期性的广播给其他节点，最终集群上所有节点都能收到消息，是一种去中心化，最终一致性的算法，压力不在来自主节点，而是所有节点都均衡负载，扩展性很好，即使有新节点加入，最终也会被广播到。

当会有个拜占庭问题，就是如果有一个恶意传播节点，将会打乱原本的传播
